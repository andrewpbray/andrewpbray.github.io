---
title: 'Homework #5 Key'
author: 
output: html_document
---

 Ch. 3 exercises 4 (only the first a and b), 5 (not a), and 8
 
4. a)

```{r}
glakes <- read.table("http://www.stat.tamu.edu/~sheather/book/docs/datasets/glakes.txt",
                     header = TRUE)
m1 <- lm(Time ~ Tonnage, data = glakes)
par(mfrow = c(2, 2))
plot(m1)
```

The primary problem with fitting a straigh-line regression model here on the raw
data is that there is strongly increasing variance in the residuals.  There are
also signs of two highly influential points (3 and 31).  The linear trend and the
normality of the residuals seem like they are find assumptions, however. We might
want to look into how the port operates to assess whether the observations are
independent.  We don't know the units of time, but if these are long time spans
that extend beyond blocks of operating hours, then the times could be dependent
as ships wait on other ships to finish up within a block of time.

b)  The interval would likely be too short.  In `m1`, the variance is assumed
constant, and that variance is estimated by using data from all ranges of x. We
can see in the scale-location plot that the variance in the data at the higher 
ranges of x (around 10,000) is greater than average, so there would be more 
variance in the data than in our PI.  This will result in a PI that is too short.

5. b)

```{r}
cars04 <- read.csv("http://www.stat.tamu.edu/~sheather/book/docs/datasets/cars04.csv")
m1 <- lm(SuggestedRetailPrice ~ DealerCost, data = cars04)
par(mfrow = c(2, 2))
plot(m1)
```

This is a very problematic model.  The scale location plot shows that we can 
strongly increasing variance in the residuals.  The Cook's Distance plot shows
one point in particular (223) that is highly influential.  The distribution of
the residuals has longer tails that the normal distribution, calling into question
the assumption of normally distributed errors.  Finally, there is structure in the 
residual plot that suggests that the mean function is not well modeled with a
straight line. We don't have enough information about where the data come from
to assess whether the independence assumption is reasonable.

c)

```{r}
cars04 <- transform(cars04,
                    logPrice = log(SuggestedRetailPrice),
                    logCost = log(DealerCost))
m2 <- lm(logPrice ~ logCost, cars04)
par(mfrow = c(2, 2))
plot(m2)
```

This model is a big improvement over the untransformed one.  Most notably, the 
variance has stabilized and there are no longer any influential points.

d) Two cars that are separated by 1% in their dealer cost are expected to be 
separated by `r m2$coef[2]`% in their suggested retail price.

e) The left tail of the residuals is still longer than expected, so prediction 
intervals can be expected to be off.  The most persistent problem, though, is 
the non-linear trend in the residuals, suggesting that a quadratic model might 
be an improvement.

8. **Part I**
a)

```{r}
diamonds <- read.table("http://www.stat.tamu.edu/~sheather/book/docs/datasets/diamonds.txt",
                       header = TRUE)
m1 <- lm(Price ~ Size, data = diamonds)
plot(Price ~ Size, data = diamonds)
abline(m1)
```

If we create a scatterplot to examine the relationship between the size and price
of diamonds, we see that there is a clear positive and linear relationship with
no apparent outliers.  This is a good candidate for straight-line regression.

b)

```{r}
par(mfrow = c(2, 2))
plot(m1)
```

Based on the scale location plot, there is some indication that the variance of
the errors may be increasing with x.  The residul plot shows no strong patterns,
suggesting that the assumption of linearity is OK.  Similarly, with the QQ plot
the indicates that the normal error assumption is warranted.  There are no highly
influential points found in the Cook's distance plot. It's likely that each diamond
is independent of the others unless the prices were set to add up to some fixed
amount.

**Part II*

a)

```{r}
diamonds <- transform(diamonds,
                      logSize = log(Size),
                      sqrtSize = sqrt(Size),
                      logPrice = log(Price),
                      sqrtPrice = sqrt(Price))
plot(logPrice ~ Size, data = diamonds)
m2 <- lm(logPrice ~ logSize, data = diamonds)
par(mfrow = c(2, 2))
plot(m2)
m3 <- lm(sqrtPrice ~ sqrtSize, data = diamonds)
par(mfrow = c(2, 2))
plot(m3)
```

In order to control the variance of the residuals, we consider transformations.
There are two models that seem best: the log-log and sqrt-sqrt models.  Their 
residuals plots are very similar, with no non-linearity in the residual plot, fairly
normal residuals, and no influential points.  Models in which one the predictor 
or the response were transform led to residual plots that shows a non-linear 
trend and the variance problem remained.

b) Both transformed models continue to have some issues with constant variance.
Specifically, there is a spike around a fitted value of around 450 that persists
even in these models.


** Part III**
Neither of the models (or actually all three) that we considered is clearly
better than the others.  All of them may have some issues with constant variance.
The transformed models may be *slightly* improved in that regard, so they might
be preferred.


