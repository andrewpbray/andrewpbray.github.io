---
title: "Simple Linear Regression"
output:
  ioslides_presentation:
    incremental: true
---


## 

Without using your book, derive expressions for $\hat{\beta}_0$ and $\hat{\beta}_1$ by
solving for them in the normal equations:

\[
\sum_{i=1}^n y_i = n\hat{\beta}_0 + \hat{\beta}_1 \sum_{i=1}^n x_i \\
\sum_{i=1}^n x_i y_i = \hat{\beta}_0 \sum_{i=1}^n x_i + \hat{\beta}_1 \sum_{i=1}^n x_i^2
\]

*note*: $\frac{1}{n}\sum_{i=1}^n x_i$ can be rewritten $\bar{x}$.

## Poverty and Graduation

```{r echo=FALSE}
poverty <- read.delim("http://andrewpbray.github.io/poverty.txt", header = TRUE)
plot(poverty$Poverty, poverty$Graduates, pch = 16, col = "steelblue")
```


## Which line?

```{r, eval=FALSE}
download.file("http://www.openintro.org/stat/data/mlb11.RData",
              destfile = "mlb11.RData")
load("mlb11.RData")
plot_ss(poverty$Poverty, poverty$Graduates)
```


## Linear models in R {.build}

The workhorse function: `lm()`

```{r, eval=TRUE}
m1 <- lm(Graduates ~ Poverty, data = poverty)
```

The formula notation is read: "I'd like to express y as a function of x". It 
creates a rich object of class `lm`.

```{r}
class(m1)
names(m1)
```


## Linear models in R {.build}

You can extract info about your model using

1. **Attributes**: reference the attributes found with `names()` by using the `$`
operator.

2. **Summary**: the most useful information can be displayed using the `summary()`
command.

3. **Print**: print your model object to get the basic coefficient estimates.


## summary(m1)

```{r, echo=FALSE, size='footnote'}
summary(m1)
```


## Slope interpretation {.build}

The slope, $\beta_1$, tells you that a one unit increase in the $x$ is associated
with a $\beta_1$ unit increase in the $y$, on average.

A one percentage point increase in a state's poverty rate is associated with a 
`r m1$coef[2]` decrease in the state's graduation rate, on average.

Why the "on average"?

\[ \hat{E}(Y|X) = \hat{\beta}_0 + \hat{\beta}_1 * x \]

*note*: sign of $\beta_1$ is often more interesting than magnitude (effected by
scaling).


## Intercept interpretation {.build}

**Mathematically**: The expected value of $y$ when $x$ is zero.

**Contextually**: Sometimes, the "start-up" value of the $y$.

Does the slope have meaning in the poverty vs grad rate example?

<div class="centered">
*Not really*.
</div>


## Activity 3: SLR on quakes

<div class="centered">
![fiji](http://www.widescenes.com/Fiji%20Gallery%20Images/Fiji2115_Large.jpg)
</div>

Recall the last question from homework 1: would you expect a relationship between 
the magnitude of an earthquake and the number of stations that detect it?


## Inference on Regression


## Plato's Allegory of the Cave

<div class="centered">
![plato](http://4.bp.blogspot.com/-rV1c4Xh4SSE/UZshhTTdFsI/AAAAAAAACQA/1VkmOaF7WFU/s1600/plato-cave.jpg)
</div>


## Statistical Inference {.build}

**Goal**: use *statistics* calculated from data to makes inferences about the 
nature of *parameters*.

In regression,

- statistics: $\hat{\beta}_0$, $\hat{\beta}_1$
- parameters: $\beta_0$, $beta_1$

Classical tools of inference:

- Confidence Intervals
- Hypothesis Tests


## Confidence Intervals {.build}

A confidence interval expresses the amount of uncertainly we have in our estimate
of a particular parameter.  A general 1 - $\alpha$ confidence interval takes the form

\[ \hat{\theta} \pm t^{*} * SE(\hat{\theta}) \]

- $\alpha$: is the confidence level, often .05
- $\hat{\theta}$: a statistic (point estimate)
- $t^{*}$ is the $100(1 - \alpha / 2)$ quantile of the sampling distribution of $\hat{\theta}$
- $SE$ is the standard error of $\hat{\theta}$, i.e. the standard deviation of its sampling
distribution.


## Regression Assumptions {.build}

1. $Y$ is related to $x$ by a simple linear regression model.
\[ E(Y|X) = \beta_0 + \beta_1 * x \]

2. The errors $e_1, e_2, \ldots, e_n$ are independent of one another.

3. The errors have a common variance $\sigma^2$.

4. The errors are normally distributed: $e \sim N(0, \sigma^2)$


## The Sampling Distribution of $\hat{\beta}$

Let's assume the following model as true:

\[ E(Y|X) = 12 + .7 * x]; e \sim N(0, 4) \]

```{r, echo=FALSE,eval=FALSE}
n <- 60
beta_0 <- 12
beta_1 <- .7
sigma <- 2

plot(20, 25, xlim = c(12, 28), ylim = c(17, 35), ylab = "y", xlab = "x", type = "n") # set up an empty plot
abline(a = beta_0, b = beta_1, col = "orange", lwd = 2) # add mean function

# generate data
x <- rnorm(n, mean = 20, sd = 3)
f_mean <- beta_0 + beta_1 * x # mean function
f_data <- f_mean + rnorm(n, mean = 0, sd = sigma) # data generating function

points(x, f_data, pch = 16, col = "steelblue") # add generated data
m1 <- lm(f_data ~ x)
m1
abline(m1)
```


## The Sampling Distribution of $\hat{\beta}$

```{r, echo=FALSE,eval=FALSE}
it <- 5000
coef_mat <- matrix(rep(NA, it * 2), ncol = 2)
for(i in 1:it) {
  x <- rnorm(n, mean = 20, sd = 3)
  f_mean <- beta_0 + beta_1 * x
  f_data <- f_mean + rnorm(n, mean = 0, sd = sigma)
  coef_mat[i, ] <- lm(f_data ~ x)$coef
}

beta_1s <- coef_mat[, 2]
hist(beta_1s)
mean(beta_1s)
sd(beta_1s)

betas_t <- (beta_1s - mean(beta_1s))/sd(beta_1s)
hist(betas_t, freq = FALSE)
xl <- seq(min(betas_t), max(betas_t), length.out = 100)
lines(xl, dnorm(xl), lwd = 2, col = "orange")
```


## The Sampling Distribution of $\hat{\beta_1}$

Characteristics:

1. Centered at $\beta_1$, i.e. $E(\hat{\beta}_1) = \beta$.
2. $Var(\hat{\beta}_1) = \frac{\sigma^2}{SXX}$.
3. $\hat{\beta}_1 | X \sim N (\beta_1, \frac{\sigma^2}{SXX})$.


## Approximating the Sampling Distribution of $\hat{\beta_1}$

Our best guess of $\beta_1$ is $\hat{\beta}_1$. And since we have to estimate
$\sigma$ with $\sigma^a = RSS/n-2$, the distribution isn't normal, but...

T with n - 2 degrees of freedom.


## Constructing a CI for $\hat{\beta_1}$

\[ \hat{\beta}_1 \pm t_{\alpha/2, n-2} * SE(\hat{\beta}_1) \]

```{r, eval=FALSE}
beta_1 <- m1$coef[2]
alpha <- .05
t_stat <- qt(1-alpha/2, n - 2)
moe <- t_stat * SE
c(beta_1 - moe, beta_1 + moe)

confint(m1, "x")
```


## Interpreting a CI for $\hat{\beta_1}$

We are *95% confident* that the true slope between x and y lies between LB and UB.
